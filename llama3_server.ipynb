{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok transformers torch PyPDF2 pandas --quiet"
      ],
      "metadata": {
        "id": "H0UXmi1YAjhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8db262-e422-42d0-ef5a-27ea873f0a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzjtmfg_B83A",
        "outputId": "7f1b806a-f5c6-40b4-f142-7c378fdb823f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "The token `llama3-token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llama3-token`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 30DWmXnyTPR9NsYZ3YNfBSkPkgK_LteWuSr3gPC4mMmRUqGy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3XU0lIiCoX3",
        "outputId": "96dda980-1f03-47e3-ccd1-e93edc716843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a93c31647a08499b8bc360771a1b029e"
          ]
        },
        "id": "DQu-hy7mVaXk",
        "outputId": "c87efecd-7873-467c-d13c-6cd620f0bea9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a93c31647a08499b8bc360771a1b029e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded!\n",
            " * ngrok tunnel URL: NgrokTunnel: \"https://87b992f4dff5.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Jul/2025 21:31:41] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Jul/2025 21:34:04] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [22/Jul/2025 21:51:50] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Start the backend server (LLaMA 3 + Flask + ngrok)\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pyngrok import ngrok\n",
        "import torch, PyPDF2, pandas as pd, io, csv\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load tokenizer and model on GPU\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "def parse_pdf(file_stream):\n",
        "    reader = PyPDF2.PdfReader(file_stream)\n",
        "    return \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "\n",
        "def parse_csv(file_stream):\n",
        "    df = pd.read_csv(file_stream)\n",
        "    return \"\\n\".join([f\"{row['Patient']}\\n{row['Therapist']}\" for _, row in df.iterrows()])\n",
        "\n",
        "def build_prompt(book_text, starter_dialogue):\n",
        "    return f\"\"\"\n",
        "You are a professional therapist. Based on the following therapy guide and ongoing dialogue, continue the conversation empathetically and realistically.\n",
        "\n",
        "--- Therapy Guide ---\n",
        "{book_text}\n",
        "\n",
        "--- Conversation so far ---\n",
        "{starter_dialogue}\n",
        "\n",
        "--- Continue the conversation starting as Therapist. Respond with realistic alternating lines between Patient and Therapist until a natural stopping point. ---\n",
        "\"\"\"\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    try:\n",
        "        pdf_file = request.files[\"pdf\"]\n",
        "        csv_file = request.files[\"csv\"]\n",
        "\n",
        "        book_text = parse_pdf(pdf_file.stream)\n",
        "        starter_dialogue = parse_csv(csv_file.stream)\n",
        "        prompt = build_prompt(book_text, starter_dialogue)\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response_text = generated[len(prompt):].strip()\n",
        "\n",
        "        # Turn into alternating dialogue CSV\n",
        "        lines = response_text.split(\"\\n\")\n",
        "        data = []\n",
        "        for i in range(0, len(lines), 2):\n",
        "            patient = lines[i].strip()\n",
        "            therapist = lines[i+1].strip() if i+1 < len(lines) else \"\"\n",
        "            data.append([patient, therapist])\n",
        "\n",
        "        output_path = \"/tmp/generated_dialogue.csv\"\n",
        "        with open(output_path, \"w\", newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Patient\", \"Therapist\"])\n",
        "            writer.writerows(data)\n",
        "\n",
        "        return send_file(output_path, as_attachment=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"response\": f\"Error in Colab: {str(e)}\"}), 500\n",
        "\n",
        "# Start ngrok and Flask\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\" * ngrok tunnel URL:\", public_url)\n",
        "app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Da7-0H9gCtcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}